{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIo6K4xykfRb",
        "outputId": "ffa00a9e-11aa-4ff3-b24d-863f241f2ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=6833bdf38603d0788b3956a7af3acc056e0c367919567d0bcc85c4bdf81592e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m934.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "!pip install langdetect\n",
        "!pip install umap-learn\n",
        "\n",
        "from langdetect import detect\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from umap import UMAP\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeXQuILIkm2M",
        "outputId": "ed4b3815-127d-4ca1-ddeb-941209627292"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data and tokenization"
      ],
      "metadata": {
        "id": "W892_XWLkp7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(dato_csv):\n",
        "    \"\"\"\n",
        "    This function cleans the input DataFrame and ouput DataFrame.\n",
        "    Remove Missing Values: Drops rows with any missing values.\n",
        "    Convert to String: Ensures all data is treated as strings for consistency.\n",
        "    Remove Duplicates: Drops duplicate rows.\n",
        "    Filter Short Texts: Removes rows where the length of the text is less than 20 characters.\n",
        "    Convert to Lowercase: Converts all text to lowercase.\n",
        "    Remove Special Characters: Removes any characters that are not words or whitespace N = 20.\n",
        "    \"\"\"\n",
        "    dato_csv = dato_csv.dropna()  # Remove rows with missing values\n",
        "    dato_csv = dato_csv.astype(str)  # Convert all data to string\n",
        "    dato_csv = dato_csv.drop_duplicates()  # Remove duplicate rows\n",
        "    dato_csv = dato_csv[dato_csv['text'].str.len() >= 20]  # Remove rows where 'text' length is less than 20\n",
        "\n",
        "    # esto ya lo hace por defecto tfidvecotirzer:\n",
        "    #for column in dato_csv.columns:\n",
        "        #dato_csv[column] = dato_csv[column].str.lower()  # Convert to lowercase-->done dirctly by TFIDvectorizer\n",
        "       # dato_csv[column] = dato_csv[column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Remove special characters\n",
        "\n",
        "    return dato_csv"
      ],
      "metadata": {
        "id": "TFYv4rfxkrOt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths for input CSV files\n",
        "\n",
        "file_path_test = 'data/test.csv'\n",
        "file_path_train = 'data/train.csv'\n",
        "\n",
        "\n",
        "#file_path_test = '/content/drive/My Drive/DATALAB/test.csv'\n",
        "\n",
        "#file_path_train = '/content/drive/My Drive/DATALAB/train.csv'\n",
        "\n",
        "\n",
        "# Read CSV files\n",
        "df_test = pd.read_csv(file_path_test)\n",
        "df_train = pd.read_csv(file_path_train)\n",
        "\n",
        "# Clean data\n",
        "df_test = clean_data(df_test)\n",
        "df_train = clean_data(df_train)\n",
        "\n",
        "\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMhzea-IkyTY",
        "outputId": "4ad4e735-6e49-42d5-9d85-39fa2693ca74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  id                                              title              author  \\\n",
            "0  0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
            "1  1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
            "2  2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
            "3  3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
            "4  4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
            "\n",
            "                                                text label  \n",
            "0  House Dem Aide: We Didn’t Even See Comey’s Let...     1  \n",
            "1  Ever get the feeling your life circles the rou...     0  \n",
            "2  Why the Truth Might Get You Fired October 29, ...     1  \n",
            "3  Videos 15 Civilians Killed In Single US Airstr...     1  \n",
            "4  Print \\nAn Iranian woman has been sentenced to...     1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We keep only the text in English"
      ],
      "metadata": {
        "id": "Ypnz9iDuk1vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect and filter only English text from a corpus and edit the original DataFrame\n",
        "def filter_english_text_edit_df(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    # Initialize an empty list to store indices of rows to keep\n",
        "    keep_indices = []\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[text_column]\n",
        "        try:\n",
        "            # Detect the language of the text\n",
        "            if detect(text) == 'en':\n",
        "                # If the language is English, add the index to the list of keep_indices\n",
        "                keep_indices.append(index)\n",
        "        except:\n",
        "            # If language detection fails (raises an exception), skip the text\n",
        "            pass\n",
        "\n",
        "    # Filter the original DataFrame to keep only the rows where text is in English\n",
        "    filtered_df = df.loc[keep_indices].reset_index(drop=True)\n",
        "\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "OvIh1tmyk3Ol"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VECTORIZATION: Language detection function and TF-IDF vectors.\n",
        "We save the unigrams TF-IDF vectors (single words) and bigrams TF-IDF vectors (pairs of consecutive words). We create visualizations to show the distribution of the vectors and how the unigrams and bigrams are repeated in the case of fake or real news."
      ],
      "metadata": {
        "id": "zVjp9hwNk-Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to vectorize text using TF-IDF\n",
        "def get_tfidf_vectors(corpus: np.ndarray, stop_words: str, max_features: int, n: int) -> np.ndarray:\n",
        "    # Create a TfidfVectorizer object with the given parameters:\n",
        "    # - stop_words: language for stop words (e.g., 'english') or None to include all words\n",
        "    # - max_features: maximum number of features (terms) to consider when vectorizing\n",
        "    # - ngram_range: range of n-grams to consider; (n, n) means only n-grams of size 'n'\n",
        "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features, ngram_range=(n, n))\n",
        "\n",
        "    # Fit the vectorizer to the corpus and transform the text data into TF-IDF vectors\n",
        "    vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Return the resulting TF-IDF vectors\n",
        "    return vectorized"
      ],
      "metadata": {
        "id": "N0U55aR9lAEk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to only include English text\n",
        "df_train_en = filter_english_text_edit_df(df_train, 'text')\n",
        "\n",
        "\n",
        "labels1 = df_train_en.copy()['label'].values\n",
        "labels = [int(label) for label in labels1]\n",
        "filtered_corpus = df_train_en.copy()['text'].values\n",
        "\n",
        "max_features = 30  #esto se puede cambiar\n",
        "\n",
        "unigram_vectors_without_stopwords = get_tfidf_vectors(filtered_corpus, 'english', max_features, 1)\n",
        "bigram_vectors_without_stopwords = get_tfidf_vectors(filtered_corpus, 'english', max_features, 2)\n"
      ],
      "metadata": {
        "id": "fUEUzsbulF5Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separo en train y validation: Uso X=los vectores que calcule a partir de la amtrix Tfid (en este caso unigramas).\n",
        "# y como 'y' a los labels.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(unigram_vectors_without_stopwords, labels, random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "#print(f'y_train shape: {y_train.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzFOskRjlLiy",
        "outputId": "8a89bc8e-b690-474a-9f1a-5c7fc0f01efc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (13420, 30)\n",
            "X_test shape: (4474, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar el modelo y Evaluar métricas\n"
      ],
      "metadata": {
        "id": "FVtHf1vdlOKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score # Import precision_score, recall_score, and f1_score"
      ],
      "metadata": {
        "id": "3xe1HdCLldfv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "pR5MENrclgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Naoo2uFSlheK",
        "outputId": "f581784c-a282-4b25-947b-3f043bf5e702"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8220831470719714\n",
            "Precision: 0.7690315898498188\n",
            "Recall: 0.8092643051771117\n",
            "F1 Score: 0.7886351566648964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "71dGyOTplm33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTEefNoZloCq",
        "outputId": "d7a96a32-3d4f-412f-aead-de3b085724f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8430934286991506\n",
            "Precision: 0.8034279592929834\n",
            "Recall: 0.8174386920980926\n",
            "F1 Score: 0.8103727714748784\n"
          ]
        }
      ]
    }
  ]
}